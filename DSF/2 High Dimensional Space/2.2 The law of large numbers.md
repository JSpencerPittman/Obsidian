When one generates points in $d$-dimensional space using a Gaussian to generate coordinates, the distance between all pair of points will be essentially the same when $d$ is large. The reason is that the square of the distance between two points $y$ and $z$,
$$\Large |y-z|^2=\sum^d_{i=1}(y_i-z_i)^2$$
can be viewed as the sum of $d$ independent samples of a random variable $x$ such that $x_i=(y_i-z_i)^2$ of a random variable of bound variance. In such a case, a general bound known as the **Law of large numbers** states that with high probability, the average of the samples will be close to the expectation of the random variable. This in turn implies that the sum is close to the sum's expectation. 

Specifically the law of large number states that 

*Equation 2.1, Law of large numbers*
$$\Large Prob(|\frac{x_1+x_2+\cdots+x_n}{n}-E(x)|\geq\epsilon)\leq\frac{Var(x)}{n\epsilon^2}$$
> The higher the variance the higher the probability it will exceed $\epsilon$ hence it's in the 
> 	numerator
> The higher the amount of samples the more values that are averaged resulting
> 	in a lower probability of exceeding $\epsilon$
> The larger $\epsilon$ is the harder it will be to exceed it and the squaring of it makes the 
> 	fraction a dimensionless quality

We prove this through two inequalities
1. Markov's inequality 
2. Chebyshev's inequality

### Markov's Inequality
Markov's inequality that states that the probability that a nonnegative random variable exceeds $a$ is bounded by the expected value of the variable divided by $a$

*Theorem 2.1, Markov's inequality*
for $a > 0$, and a nonnegative random variable $x$
$$
\Large P(x\geq a)=\frac{E(x)}{a}
$$
#### Proof
$$\Large E(x)=\int_0^\infty xp(x)dx\geq\int_a^\infty xp(x)dx\geq a\int_a^\infty p(x)dx=aProb(x\geq a)$$
Therefore
$$\Large E(x)\geq aProb(x\geq a) \rightarrow \frac{E(x)}{a}\geq Prob(x\geq a)$$
This applies to discrete variables as well

*Corollary 2.2*
$$\Large Prob(x\geq bE(x))\leq\frac{1}{b}$$
Merkov's inequality only uses the mean of the dataset this next inequality uses variance for a stricter bound

### Chebyshev's Inequality
Let $x$ be a random variable, Then for $c > 0$

*Equation 2.3, Chebyshev's inequality*
$$\Large P(|x-E(x)|\geq c)\leq\frac{Var(x)}{c^2}$$
#### Proof
$$\Large Prob(|x-E(x)|\geq c)=Prob(|x-E(x)|^2\geq c^2)\leq\frac{E(|x-E(x)|)^2}{c^2}=\frac{Var(x)}{c^2}$$
Here we used markov's inequality

### Combine
**Recall:** $E(x+y)=E(x)+E(y)$ & $Var(a+bx)=b^2Var(x)$ and $E(xy)=E(x)E(y)$
$Var(x)=E(x-E(x))^2$ using these we proved $Var(x+y)=Var(x)+Var(y)$

$$\Large P(|(\frac{x_1+x_2+\cdots+x_n}{n})-E(x)|\geq\epsilon)\leq\frac{Var(\frac{x_1+x_2+\cdots+x_n}{n})}{\epsilon^2}$$
$$\Large\frac{Var(\frac{x_1+x_2+\cdots+x_n}{n})}{\epsilon^2}=\frac{1}{n^2\epsilon^2}(Var(x_1)+\cdots+Var(x_n))=\frac{Var(x)}{n\epsilon^2}$$

*Theorem 2.5, Master tail bounds theorem*
$$\Large Prob(|x|\geq a)\leq3e^{-a^2/(12n\sigma^2)}$$